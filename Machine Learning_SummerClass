{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Machine Learning_SummerClass","provenance":[],"authorship_tag":"ABX9TyOvP1bHaExNJB/NJ0SHIH92"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"V_Un3qkzSMWy"},"outputs":[],"source":["# viết thuật toán cho CSDL Weather : thuật toán độ lộn xộn: Decision Tree"]},{"cell_type":"code","source":["import numpy as np\n","\n","class GadId3Classifier:\n","  def fit(self, input, output):\n","    data = input.copy()\n","    data[output.name] = output\n","    self.tree = self.decision_tree(data, data, input.columns, output.name)\n","\n","  def predict(self, input):\n","    # convert input data into a dictionary of samples\n","    samples = input.to_dict(orient='records')\n","    predictions = []\n","\n","    # make a prediction for every sample\n","    for sample in samples:\n","      predictions.append(self.make_prediction(sample, self.tree, 1.0))\n","\n","    return predictions\n","\n","  def entropy(self, attribute_column):\n","    # find unique values and their frequency counts for the given attribute\n","    values, counts = np.unique(attribute_column, return_counts=True)\n","\n","    # calculate entropy for each unique value\n","    entropy_list = []\n","\n","    for i in range(len(values)):\n","      probability = counts[i]/np.sum(counts)\n","      entropy_list.append(-probability*np.log2(probability))\n","\n","    # calculate sum of individual entropy values\n","    total_entropy = np.sum(entropy_list)\n","\n","    return total_entropy\n","  \n","  def information_gain(self, data, feature_attribute_name, target_attribute_name):\n","    # find total entropy of given subset\n","    total_entropy = self.entropy(data[target_attribute_name])\n","\n","    # find unique values and their frequency counts for the attribute to be split\n","    values, counts = np.unique(data[feature_attribute_name], return_counts=True)\n","\n","    # calculate weighted entropy of subset\n","    weighted_entropy_list = []\n","\n","    for i in range(len(values)):\n","      subset_probability = counts[i]/np.sum(counts)\n","      subset_entropy = self.entropy(data.where(data[feature_attribute_name]==values[i]).dropna()[target_attribute_name])\n","      weighted_entropy_list.append(subset_probability*subset_entropy)\n","\n","    total_weighted_entropy = np.sum(weighted_entropy_list)\n","\n","    # calculate information gain\n","    information_gain = total_entropy - total_weighted_entropy\n","\n","    return information_gain\n","\n","  def decision_tree(self, data, orginal_data, feature_attribute_names, target_attribute_name, parent_node_class=None):\n","    # base cases:\n","    # if data is pure, return the majority class of subset\n","    unique_classes = np.unique(data[target_attribute_name])\n","    if len(unique_classes) <= 1:\n","      return unique_classes[0]\n","    # if subset is empty, ie. no samples, return majority class of original data\n","    elif len(data) == 0:\n","      majority_class_index = np.argmax(np.unique(original_data[target_attribute_name], return_counts=True)[1])\n","      return np.unique(original_data[target_attribute_name])[majority_class_index]\n","    # if data set contains no features to train with, return parent node class\n","    elif len(feature_attribute_names) == 0:\n","      return parent_node_class\n","    # if none of the above are true, construct a branch:\n","    else:\n","      # determine parent node class of current branch\n","      majority_class_index = np.argmax(np.unique(data[target_attribute_name], return_counts=True)[1])\n","      parent_node_class = unique_classes[majority_class_index]\n","\n","      # determine information gain values for each feature\n","      # choose feature which best splits the data, ie. highest value\n","      ig_values = [self.information_gain(data, feature, target_attribute_name) for feature in feature_attribute_names]\n","      best_feature_index = np.argmax(ig_values)\n","      best_feature = feature_attribute_names[best_feature_index]\n","\n","      # create tree structure, empty at first\n","      tree = {best_feature: {}}\n","\n","      # remove best feature from available features, it will become the parent node\n","      feature_attribute_names = [i for i in feature_attribute_names if i != best_feature]\n","\n","      # create nodes under parent node\n","      parent_attribute_values = np.unique(data[best_feature])\n","      for value in parent_attribute_values:\n","        sub_data = data.where(data[best_feature] == value).dropna()\n","\n","        # call the algorithm recursively\n","        subtree = self.decision_tree(sub_data, orginal_data, feature_attribute_names, target_attribute_name, parent_node_class)\n","\n","        # add subtree to original tree\n","        tree[best_feature][value] = subtree\n","\n","      return tree\n","\n","  def make_prediction(self, sample, tree, default=1):\n","    # map sample data to tree\n","    for attribute in list(sample.keys()):\n","      # check if feature exists in tree\n","      if attribute in list(tree.keys()):\n","        try:\n","          result = tree[attribute][sample[attribute]]\n","        except:\n","          return default\n","\n","        result = tree[attribute][sample[attribute]]\n","\n","        # if more attributes exist within result, recursively find best result\n","        if isinstance(result, dict):\n","          return self.make_prediction(sample, result)\n","        else:\n","          return result"],"metadata":{"id":"sAZFivNv7z-V"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score\n","\n","data_url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.cleveland.data\"\n","df = pd.read_csv(data_url, header=None)\n","\n","# rename known columns\n","columns = ['age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg',\n","           'thalach', 'exang', 'oldpeak', 'slope', 'ca', 'thal', 'disease_present']\n","df.columns = columns\n","\n","# convert disease_present feature to binary\n","df['disease_present'] = df.disease_present.replace([1,2,3,4], 1)\n","\n","# drop rows with missing values, missing = ?\n","df = df.replace(\"?\", np.nan)\n","df = df.dropna()\n","\n","# organize data into input and output\n","X = df.drop(columns=\"disease_present\")\n","y = df[\"disease_present\"]\n"," \n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)\n"," \n","# initialize and fit model\n","model = GadId3Classifier()\n","model.fit(X_train, y_train)\n","\n","# return accuracy score\n","y_pred = model.predict(X_test)\n","accuracy_score(y_test, y_pred)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fd4YCF_x73Kx","executionInfo":{"status":"ok","timestamp":1658594532466,"user_tz":-420,"elapsed":12942,"user":{"displayName":"Vân Trần Thảo","userId":"05921205066487747524"}},"outputId":"942512ee-dfe5-4c6d-c187-279c635023e7"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.5959595959595959"]},"metadata":{},"execution_count":2}]},{"cell_type":"code","source":["df.disease_present.value_counts(normalize=True).max()"],"metadata":{"id":"m8Os1ArqAmd9","executionInfo":{"status":"ok","timestamp":1658595736205,"user_tz":-420,"elapsed":380,"user":{"displayName":"Vân Trần Thảo","userId":"05921205066487747524"}},"outputId":"bb850cd5-14dd-47d3-d68d-490b12208bb9","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.5387205387205387"]},"metadata":{},"execution_count":3}]},{"cell_type":"code","source":["# Cách khác"],"metadata":{"id":"xffhYOVOPbI4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def find_entropy(df):\n","    Class = df.keys()[-1]   #To make the code generic, changing target variable class name\n","    entropy = 0\n","    values = df[Class].unique()\n","    for value in values:\n","        fraction = df[Class].value_counts()[value]/len(df[Class])\n","        entropy += -fraction*np.log2(fraction)\n","    return entropy\n","def find_entropy_attribute(df,attribute):\n","    Class = df.keys()[-1]   #To make the code generic, changing target variable class name\n","    target_variables = df[Class].unique()  #This gives all 'Yes' and 'No'\n","    variables = df[attribute].unique()    #This gives different features in that attribute (like 'Hot','Cold' in Temperature)\n","    entropy2 = 0\n","    for variable in variables:\n","        entropy = 0\n","        for target_variable in target_variables:\n","            num = len(df[attribute][df[attribute]==variable][df[Class] ==target_variable])\n","            den = len(df[attribute][df[attribute]==variable])\n","            fraction = num/(den+eps)\n","            entropy += -fraction*log(fraction+eps)\n","        fraction2 = den/len(df)\n","        entropy2 += -fraction2*entropy\n","    return abs(entropy2)\n","def find_winner(df):\n","    Entropy_att = []\n","    IG = []\n","    for key in df.keys()[:-1]:#         Entropy_att.append(find_entropy_attribute(df,key))\n","        IG.append(find_entropy(df)-find_entropy_attribute(df,key))\n","    return df.keys()[:-1][np.argmax(IG)] \n","def get_subtable(df, node,value):\n","    return df[df[node] == value].reset_index(drop=True)\n","def buildTree(df,tree=None): \n","    Class = df.keys()[-1]   #To make the code generic, changing target variable class name  #Here we build our decision tree  #Get attribute with maximum information gain\n","    node = find_winner(df)#Get distinct value of that attribute e.g Salary is node and Low,Med and High are values\n","    attValue = np.unique(df[node])#Create an empty dictionary to create tree    \n","    if tree is None:                    \n","        tree={}\n","        tree[node] = {}#We make loop to construct a tree by calling this function recursively. #In this we check if the subset is pure and stops if it is pure. \n","    for value in attValue:\n","        subtable = get_subtable(df,node,value)\n","        clValue,counts = np.unique(subtable['play'],return_counts=True)                        \n","        if len(counts)==1:#Checking purity of subset\n","            tree[node][value] = clValue[0]                                                    \n","        else:        \n","            tree[node][value] = buildTree(subtable) #Calling the function recursively               \n","    return tree\n","import pandas as pd\n","import numpy as np\n","eps = np.finfo(float).eps\n","from numpy import log2 as log\n","df = pd.read_csv('tennis.csv')\n","print(\"\\n Given Play Tennis Data Set:\\n\\n\",df)\n","tree= buildTree(df)\n","import pprint\n","pprint.pprint(tree)\n","\n","test={'Outlook':'Sunny','Temperature':'Hot','Humidity':'High','Wind':'Weak'}\n","def func(test, tree, default=None):\n","    attribute = next(iter(tree)) \n","    print(attribute) \n","    if test[attribute] in tree[attribute].keys():\n","        print(tree[attribute].keys())\n","        print(test[attribute])\n","        result = tree[attribute][test[attribute]]\n","        if isinstance(result, dict):\n","            return func(test, result)\n","        else:\n","            return result\n","    else:\n","        return default\n","ans = func(test, tree)\n","print(ans)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":380},"id":"Y5q87bY_Pdjk","executionInfo":{"status":"error","timestamp":1658650061255,"user_tz":-420,"elapsed":7,"user":{"displayName":"Vân Trần Thảo","userId":"05921205066487747524"}},"outputId":"8748eeb3-b69b-42fd-d7b1-8f7226a50041"},"execution_count":1,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-101ffe6a28ef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0meps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlog2\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mlog\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tennis.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n Given Play Tennis Data Set:\\n\\n\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0mtree\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mbuildTree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1038\u001b[0m             )\n\u001b[1;32m   1039\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m    227\u001b[0m             \u001b[0mmemory_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"memory_map\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m             \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"storage_options\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m             \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"encoding_errors\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"strict\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m         )\n\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    705\u001b[0m                 \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 707\u001b[0;31m                 \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    708\u001b[0m             )\n\u001b[1;32m    709\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'tennis.csv'"]}]}]}